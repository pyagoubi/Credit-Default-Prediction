{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOOdZDzIItEG",
        "outputId": "c1979083-88c8-4c3a-fbe2-4ba3b8e899af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lJRhid0ItBS"
      },
      "outputs": [],
      "source": [
        "#set WD\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Amex/parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMo1TkPZp0N4"
      },
      "source": [
        "# Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRgokt4bp0N4"
      },
      "outputs": [],
      "source": [
        "# LOAD LIBRARIES\n",
        "import pandas as pd, numpy as np # CPU libraries\n",
        "import matplotlib.pyplot as plt, gc, os\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBClassifier\n",
        "import csv, itertools\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import plot_importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sct61n7Yp0N5"
      },
      "outputs": [],
      "source": [
        "# VERSION NAME FOR SAVED MODEL FILES\n",
        "VER = 12\n",
        "\n",
        "# TRAIN RANDOM SEED\n",
        "SEED = 42\n",
        "\n",
        "# FILL NAN VALUE\n",
        "NAN_VALUE = -127 # will fit in int8\n",
        "\n",
        "# FOLDS PER MODEL\n",
        "FOLDS = 5\n",
        "\n",
        "NUM_PARTS = 10\n",
        "\n",
        "\n",
        "TRAIN_PATH = 'train.parquet'\n",
        "TEST_PATH = 'test.parquet'\n",
        "TARGET_PATH = 'train_labels.csv'\n",
        "SAVE_PATH = '/content/drive/MyDrive/Amex/parquet/XGB final/'\n",
        "SUBMISSION_FILE_PATH = '/content/drive/MyDrive/Amex/parquet/sample_submission.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEV9Z8ZIp0N6"
      },
      "source": [
        "# Process and Feature Engineer Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWJevsrgp0N6"
      },
      "outputs": [],
      "source": [
        "def read_file(path = '', usecols = None):\n",
        "    # LOAD DATAFRAME\n",
        "    if usecols is not None: df = pd.read_parquet(path, columns=usecols)\n",
        "    else: df = pd.read_parquet(path) #df = cudf.read_parquet(path)\n",
        "    df['customer_ID'] = df['customer_ID'].str[-16:].apply(int, base =16)\n",
        "    df.S_2 = pd.to_datetime( df.S_2 )\n",
        "    print('shape of data:', df.shape)\n",
        "    return df\n",
        "\n",
        "def revertnan(df):\n",
        "  df[df==-1] = np.nan \n",
        "  return df\n",
        "\n",
        "def fill_na(df, NAN_VALUE):\n",
        "  df = df.fillna(NAN_VALUE)\n",
        "  return df\n",
        "\n",
        "def numberobs_feature(df):\n",
        "  df['number_of_observations'] = df.groupby('customer_ID')['customer_ID'].transform('count')\n",
        "  df.loc[df['B_33'].isnull() & (df.number_of_observations==1),'number_of_observations'] = 0.5\n",
        "  return df\n",
        "\n",
        "def afterpay(df):\n",
        "  # compute \"after pay\" features\n",
        "  for bcol in [f'B_{i}_last' for i in [11,14,17]]+['D_39_last','D_131_last']+[f'S_{i}_last' for i in [16,23]]:\n",
        "    for pcol in ['P_2_last','P_3_last']:\n",
        "      if bcol in df.columns:\n",
        "        df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
        "  return df\n",
        "\n",
        "def get_features(df):\n",
        "  all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n",
        "  cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
        "  num_features = [col for col in all_cols if col not in cat_features]\n",
        "  return all_cols, cat_features, num_features\n",
        "\n",
        "def agg_functions(df, num_features, cat_features, numberobs = False#, exclnullCols, \n",
        "                  #dummy_nan_col\n",
        "                  ):\n",
        "  \n",
        "  test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'max', 'min', 'last', 'first'])\n",
        "\n",
        "  print('num agg complete')\n",
        "  \n",
        "  test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "  print('cat agg complete')\n",
        "\n",
        "  df = pd.concat([test_num_agg, test_cat_agg],   #dummy_nan_col_agg, \n",
        "                   # test_nan_agg], \n",
        "                   axis=1)\n",
        "  \n",
        "  print('concat complete')\n",
        "  df.columns = ['_'.join(x) for x in df.columns]\n",
        "\n",
        "  print('drop numberobs')\n",
        "\n",
        "  if numberobs ==True:\n",
        "    to_drop = ['number_of_observations_mean', 'number_of_observations_std', 'number_of_observations_max','number_of_observations_min', 'number_of_observations_first']\n",
        "    df.drop(to_drop, axis = 1, inplace = True)\n",
        "    df.rename(columns={'number_of_observations_last':'number_of_observations'}, inplace = True)\n",
        "\n",
        "  print('drop numberobs complete')\n",
        "  del test_num_agg, test_cat_agg\n",
        "  _ = gc.collect()\n",
        "  print('shape after engineering', df.shape )\n",
        "  return df\n",
        "\n",
        "def add_meandev(df, num_features):\n",
        "  \n",
        "  for i in [f for f in num_features if f not in ['number_of_observations']]:\n",
        "    last = f'{i}_last'\n",
        "    mean = f'{i}_mean' \n",
        "    df[f'{i}_meandev'] = np.nan\n",
        "    df.loc[(df[last] != np.nan), f'{i}_meandev'] = df[last] -df[mean]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def add_targets(df, TARGET_PATH):\n",
        "  # ADD TARGETS\n",
        "  targets = pd.read_csv(TARGET_PATH)\n",
        "  targets['customer_ID'] = targets['customer_ID'].str[-16:].apply(int, base =16)\n",
        "  targets = targets.set_index('customer_ID')\n",
        "  df = df.merge(targets, left_index=True, right_index=True, how='left', sort = True)\n",
        "  df.target = df.target.astype('int8')\n",
        "  del targets\n",
        "\n",
        "  # NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
        "  df = df.reset_index()\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_difference(df, num_features, train_set = None, Part =None):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, cus in tqdm(df.groupby(['customer_ID'])):\n",
        "        # Get the differences\n",
        "        diff_df1 = cus[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        # Append to lists\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    # Concatenate\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    # Transform to dataframe\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    # Add customer id\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    df1.set_index('customer_ID', inplace = True)\n",
        "    if train_set == True: df1.to_parquet(f'{SAVE_PATH}diff_{VER}.parquet')\n",
        "    elif train_set == False: df1.to_parquet(f'{SAVE_PATH}diff_test_{VER}_num{Part}.parquet')\n",
        "    return df1\n",
        "  \n",
        "def onehot_encoding(df, cat_cols):\n",
        "  cat_cols = [f'{i}_last' for i in cat_cols]\n",
        "  for col in cat_cols:\n",
        "    df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing Pipeline\n",
        "def preprocess(PATH = TRAIN_PATH, TARGET_PATH = TARGET_PATH, train_set = True, test = None, Part = None):\n",
        "  if train_set == True:\n",
        "    df = read_file(path = TRAIN_PATH)\n",
        "  else:\n",
        "    df = test\n",
        "  print('read file complete')\n",
        "  df = revertnan(df)\n",
        "  print('revertnan complete')\n",
        "  df = numberobs_feature(df)\n",
        "  print('numberobs complete')\n",
        "  all_cols, cat_features, num_features = get_features(df)\n",
        "  print('get features complete')\n",
        "  diff1 = get_difference(df, num_features, train_set = train_set, Part = Part)\n",
        "  #if train_set == True: diff1 = pd.read_parquet(f'{SAVE_PATH}diff_{VER}.parquet')\n",
        "  print('get diff complete')\n",
        "  df = agg_functions(df, num_features, cat_features, numberobs = True)\n",
        "  df = df.merge(diff1, left_index=True, right_index=True, how='left') \n",
        "  del diff1\n",
        "  _ = gc.collect()\n",
        "  print('agg features complete')\n",
        "  df = add_meandev(df, num_features)\n",
        "  print('meandev complete')\n",
        "  df = afterpay(df)\n",
        "  print('afterpay complete')\n",
        "  df = onehot_encoding(df, cat_cols=cat_features)\n",
        "  print('onehot complete')\n",
        "  # df = add_Bratios(df)\n",
        "  df = fill_na(df, NAN_VALUE)\n",
        "  print('fillna complete')\n",
        "  gc.collect()\n",
        "  if train_set == True:\n",
        "    df = add_targets(df, TARGET_PATH)\n",
        "  return df"
      ],
      "metadata": {
        "id": "ESGxGlx23_r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = preprocess(PATH = TRAIN_PATH)\n",
        "train.to_parquet(f'{SAVE_PATH}train_{VER}.parquet')"
      ],
      "metadata": {
        "id": "wneXMBFCxsi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if already pretrained\n",
        "train = pd.read_parquet(f'{SAVE_PATH}train_{VER}.parquet')"
      ],
      "metadata": {
        "id": "rZn6fq8A6S-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_list(df):\n",
        "  features = df.columns[1:-1]\n",
        "  print(f'There are {len(features)} features!')\n",
        "  return features"
      ],
      "metadata": {
        "id": "dLSwWNVkwika"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = get_feature_list(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuPN-LUvwih1",
        "outputId": "03ee5880-74b3-4978-b291-12726a8df6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1487 features!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_features(features):\n",
        "  with open(f'{SAVE_PATH}features_V{VER}.csv', 'w') as csvfile:\n",
        "    # creating a csv writer object\n",
        "    writer = csv.writer(csvfile)    \n",
        "    writer.writerow(features) \n",
        "save_features(features)"
      ],
      "metadata": {
        "id": "UVJBCPcPFqHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del features\n",
        "features = pd.read_csv(f'{SAVE_PATH}features_V{VER}.csv')\n",
        "features = pd.Index(features.columns)"
      ],
      "metadata": {
        "id": "6z9E2agCBXwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train XGB"
      ],
      "metadata": {
        "id": "4YdNxSUbxUyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def amex_metric_mod(y_true, y_pred):\n",
        "\n",
        "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz        = cum_pos_found / total_pos\n",
        "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
        "\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)"
      ],
      "metadata": {
        "id": "vs-L3HSwybZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_xgb_parameters():\n",
        "\n",
        "  xgb_parms = {\n",
        "   'lambda': 0.19846538518330817, \n",
        "   'alpha': 0.11499421368543077, \n",
        "   'colsample_bytree': 1.0, \n",
        "   'subsample': 0.6, \n",
        "   'learning_rate': 0.01, \n",
        "   'max_depth': 8, \n",
        "   'min_child_weight': 56,\n",
        "   'eval_metric':'logloss',\n",
        "   'objective':'binary:logistic',\n",
        "   'tree_method':'gpu_hist',\n",
        "   'predictor':'gpu_predictor',\n",
        "   'random_state':SEED  \n",
        "    }\n",
        " \n",
        "  return xgb_parms"
      ],
      "metadata": {
        "id": "oGTdE15Ax1yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_parms = get_xgb_parameters()"
      ],
      "metadata": {
        "id": "3iP8P1w5x1uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(df, SEED=SEED, SAVE_PATH =SAVE_PATH, VER=VER):\n",
        "  importances = []\n",
        "  oof = []\n",
        "  TRAIN_SUBSAMPLE = 1.0\n",
        "  gc.collect()\n",
        "  kaggle_metrics_folds =[]\n",
        "\n",
        "  skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
        "  for fold,(train_idx, valid_idx) in enumerate(skf.split(\n",
        "            df, df.target )):\n",
        "    # TRAIN WITH SUBSAMPLE OF TRAIN FOLD DATA\n",
        "    if TRAIN_SUBSAMPLE<1.0:\n",
        "        np.random.seed(SEED)\n",
        "        train_idx = np.random.choice(train_idx, \n",
        "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
        "        np.random.seed(None)\n",
        "    \n",
        "    print('#'*25)\n",
        "    print('### Fold',fold+1)\n",
        "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
        "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
        "    print('#'*25)\n",
        "    \n",
        "\n",
        "\n",
        "    X_train = df.loc[train_idx, features]\n",
        "    y_train = df.loc[train_idx, 'target']\n",
        "\n",
        "    X_valid = df.loc[valid_idx, features]\n",
        "    y_valid = df.loc[valid_idx, 'target']\n",
        "\n",
        "\n",
        "    dtrain = xgb.DMatrix(data=X_train, label=y_train)   \n",
        "    dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
        "    \n",
        "    # TRAIN MODEL FOLD K\n",
        "    model = xgb.train(xgb_parms, \n",
        "                dtrain=dtrain,\n",
        "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                num_boost_round=9999,\n",
        "                early_stopping_rounds=100,\n",
        "                verbose_eval=100) \n",
        "    model.save_model(f'{SAVE_PATH}XGB_v{VER}_fold{fold}.xgb')\n",
        "    \n",
        "    # GET FEATURE IMPORTANCE FOR FOLD K\n",
        "    dd = model.get_score(importance_type='weight')\n",
        "    df_pred = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
        "    importances.append(df_pred)\n",
        "            \n",
        "    # INFER OOF FOLD K\n",
        "    oof_preds = model.predict(dvalid)\n",
        "    acc = amex_metric_mod(y_valid.values, oof_preds)\n",
        "    print('Kaggle Metric =',acc,'\\n')\n",
        "    kaggle_metrics_folds.append(acc)\n",
        "    \n",
        "    # SAVE OOF\n",
        "    df_pred = df.loc[valid_idx, ['customer_ID','target'] ].copy()\n",
        "    df_pred['oof_pred'] = oof_preds\n",
        "    oof.append( df_pred )\n",
        "    \n",
        "    del dtrain, X_train, y_train, dd, df_pred #,Xy_train,\n",
        "    del X_valid, y_valid, dvalid, model\n",
        "    _ = gc.collect()\n",
        "    \n",
        "  print('#'*25)\n",
        "  oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
        "  acc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n",
        "  print('OVERALL CV Kaggle Metric =',acc)\n",
        "  print(kaggle_metrics_folds)"
      ],
      "metadata": {
        "id": "glTabRzUx1nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train)"
      ],
      "metadata": {
        "id": "AxBFuMW5x1kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Testdata"
      ],
      "metadata": {
        "id": "-FSci_IVK90h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE SIZE OF EACH SEPARATE TEST PART\n",
        "def get_rows(customers, test, NUM_PARTS = NUM_PARTS, verbose = ''):\n",
        "    chunk = len(customers)//NUM_PARTS\n",
        "    if verbose != '':\n",
        "        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n",
        "        print(f'There will be {chunk} customers in each part (except the last part).')\n",
        "        print('Below are number of rows in each part:')\n",
        "    rows = []\n",
        "\n",
        "    for k in range(NUM_PARTS):\n",
        "        if k==NUM_PARTS-1: cc = customers[k*chunk:]\n",
        "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
        "        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
        "        rows.append(s)\n",
        "    if verbose != '': print( rows )\n",
        "    return rows,chunk"
      ],
      "metadata": {
        "id": "uDCUADjDKwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rowsnumcust(TEST_PATH, NUM_PARTS=NUM_PARTS):\n",
        "  print(f'Reading test data...')\n",
        "  test = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\n",
        "  customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
        "  rows,num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS = NUM_PARTS, verbose = 'test')\n",
        "  return rows,num_cust,customers"
      ],
      "metadata": {
        "id": "eq3hBpd44pR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows,num_cust, customers = get_rowsnumcust(TEST_PATH, NUM_PARTS=NUM_PARTS)"
      ],
      "metadata": {
        "id": "hgVJ0VjZy4By",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6fecd8-3088-4b35-e3a7-a190db9281d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading test data...\n",
            "shape of data: (11363762, 2)\n",
            "We will process test data as 10 separate parts.\n",
            "There will be 92462 customers in each part (except the last part).\n",
            "Below are number of rows in each part:\n",
            "[1136415, 1137255, 1135580, 1135734, 1136082, 1137166, 1136612, 1137228, 1136301, 1135389]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INFER TEST DATA IN PARTS\n",
        "skip_rows = 0\n",
        "skip_cust = 0\n",
        "test_preds = []\n",
        "\n",
        "for k in range(NUM_PARTS):\n",
        "  # READ PART OF TEST DATA\n",
        "  print(f'\\nReading test data...')\n",
        "  test = read_file(path = TEST_PATH)\n",
        "  test = test.sort_index()\n",
        "  test = test.iloc[skip_rows:skip_rows+rows[k]]\n",
        "  skip_rows += rows[k]\n",
        "  print(f'=> Test part {k+1} has shape', test.shape )\n",
        "    \n",
        "  # PROCESS AND FEATURE ENGINEER PART OF TEST DATA\n",
        "  test = preprocess(train_set = False, test = test, Part =k)\n",
        "  if k==NUM_PARTS-1: test = test.loc[customers[skip_cust:]]\n",
        "  else: test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n",
        "  skip_cust += num_cust\n",
        " \n",
        "   \n",
        "  #ammend for one hot encoding\n",
        "  test['D_64_last_1.0'] = 0\n",
        "  test['D_66_last_1.0'] = 0\n",
        "  test['D_68_last_1.0'] = 0\n",
        "  # TEST DATA FOR XGB\n",
        "  X_test = test[features]\n",
        "  print('X_test complete')\n",
        "  dtest = xgb.DMatrix(data=X_test)\n",
        "  print('dtest complete')\n",
        "  #test = test[['P_2_mean']] # reduce memory\n",
        "  del test, X_test\n",
        "  gc.collect()\n",
        "  gc.collect()\n",
        "\n",
        "  # INFER XGB MODELS ON TEST DATA\n",
        "  model = xgb.Booster()\n",
        "  model.load_model(f'{SAVE_PATH}XGB_v{VER}_fold0.xgb')\n",
        "  print('load model complete')\n",
        "  preds = model.predict(dtest)\n",
        "  print('preds complete')\n",
        "  for f in range(1,FOLDS):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    model = xgb.Booster()\n",
        "    model.load_model(f'{SAVE_PATH}XGB_v{VER}_fold{f}.xgb')\n",
        "    print(f'load {f} complete')\n",
        "    preds += model.predict(dtest)\n",
        "    print(f'preds {f} complete')\n",
        "  preds /= FOLDS\n",
        "  test_preds.append(preds)\n",
        "\n",
        "  # CLEAN MEMORY\n",
        "  del dtest, model\n",
        "  _ = gc.collect()\n",
        "\n",
        "test_preds = np.concatenate(test_preds)\n",
        "test = pd.DataFrame(index=customers,data={'prediction':test_preds})\n",
        "sub = pd.read_csv(SUBMISSION_FILE_PATH)[['customer_ID']]\n",
        "sub['customer_ID_hash'] = sub['customer_ID'].str[-16:].apply(int, base =16)\n",
        "sub = sub.set_index('customer_ID_hash')\n",
        "sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n",
        "sub = sub.reset_index(drop=True)\n",
        "\n",
        "sub.to_csv(f'{SAVE_PATH}submission_xgb_v{VER}.csv',index=False)\n",
        "print('Submission file shape is', sub.shape )\n",
        "sub.head()"
      ],
      "metadata": {
        "id": "8pnE74mOdQVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Amex XGBoost fin_pd_V12.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}